{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Namespace(batch_size=64, dtype='float32', epochs=5, lr=0.01, momentum=0.9, no_cuda=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loading dataset...\n",
      "Loading dataset...\n",
      "Train Batch Sampler:\n",
      "FixedBucketSampler:\n",
      "  sample_num=30, batch_num=14\n",
      "  key=[(9, 10), (11, 11), (12, 12), (13, 14), (14, 15), (16, 17), (17, 19), (19, 21), (21, 23), (27, 30), (31, 35), (35, 40), (40, 46), (47, 54)]\n",
      "  cnt=[1, 1, 1, 1, 2, 2, 2, 4, 1, 2, 5, 1, 4, 3]\n",
      "  batch_size=[270, 245, 225, 192, 183, 164, 146, 143, 122, 92, 84, 75, 64, 54]\n",
      "Validation Batch Sampler:\n",
      "FixedBucketSampler:\n",
      "  sample_num=30, batch_num=14\n",
      "  key=[9, 10, 12, 14, 15, 17, 19, 21, 23, 30, 35, 40, 46, 54]\n",
      "  cnt=[1, 1, 1, 2, 3, 3, 2, 1, 1, 4, 3, 5, 2, 1]\n",
      "  batch_size=[28, 25, 21, 18, 17, 14, 14, 12, 11, 8, 7, 6, 5, 4]\n",
      "Test Batch Sampler:\n",
      "FixedBucketSampler:\n",
      "  sample_num=30, batch_num=14\n",
      "  key=[9, 10, 12, 14, 15, 17, 19, 21, 23, 30, 35, 40, 46, 54]\n",
      "  cnt=[1, 1, 1, 2, 3, 3, 2, 1, 1, 4, 3, 5, 2, 1]\n",
      "  batch_size=[28, 25, 21, 18, 17, 14, 14, 12, 11, 8, 7, 6, 5, 4]\n",
      "Length of train_data_loader: 14\n",
      "Length of val_data_loader: 14\n",
      "Length of test_data_loader: 14\n",
      "NMTModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "    (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "    (transformer_cells): HybridSequential(\n",
      "      (0): TransformerEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (proj): Dense(None -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "      )\n",
      "      (1): TransformerEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (proj): Dense(None -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "      )\n",
      "      (2): TransformerEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (proj): Dense(None -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "      )\n",
      "      (3): TransformerEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (proj): Dense(None -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "      )\n",
      "      (4): TransformerEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (proj): Dense(None -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "      )\n",
      "      (5): TransformerEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (proj): Dense(None -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "    (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "    (transformer_cells): HybridSequential(\n",
      "      (0): TransformerDecoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell_in): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (attention_cell_inter): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (proj_in): Dense(None -> 512, linear)\n",
      "        (proj_inter): Dense(None -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm_in): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "        (layer_norm_inter): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "      )\n",
      "      (1): TransformerDecoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell_in): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (attention_cell_inter): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (proj_in): Dense(None -> 512, linear)\n",
      "        (proj_inter): Dense(None -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm_in): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "        (layer_norm_inter): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "      )\n",
      "      (2): TransformerDecoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell_in): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (attention_cell_inter): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (proj_in): Dense(None -> 512, linear)\n",
      "        (proj_inter): Dense(None -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm_in): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "        (layer_norm_inter): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "      )\n",
      "      (3): TransformerDecoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell_in): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (attention_cell_inter): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (proj_in): Dense(None -> 512, linear)\n",
      "        (proj_inter): Dense(None -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm_in): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "        (layer_norm_inter): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "      )\n",
      "      (4): TransformerDecoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell_in): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (attention_cell_inter): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (proj_in): Dense(None -> 512, linear)\n",
      "        (proj_inter): Dense(None -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm_in): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "        (layer_norm_inter): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "      )\n",
      "      (5): TransformerDecoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell_in): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (attention_cell_inter): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (proj_in): Dense(None -> 512, linear)\n",
      "        (proj_inter): Dense(None -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm_in): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "        (layer_norm_inter): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (one_step_ahead_decoder): TransformerOneStepDecoder(\n",
      "    (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "    (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "    (transformer_cells): HybridSequential(\n",
      "      (0): TransformerDecoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell_in): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (attention_cell_inter): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (proj_in): Dense(None -> 512, linear)\n",
      "        (proj_inter): Dense(None -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm_in): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "        (layer_norm_inter): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "      )\n",
      "      (1): TransformerDecoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell_in): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (attention_cell_inter): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (proj_in): Dense(None -> 512, linear)\n",
      "        (proj_inter): Dense(None -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm_in): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "        (layer_norm_inter): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "      )\n",
      "      (2): TransformerDecoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell_in): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (attention_cell_inter): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (proj_in): Dense(None -> 512, linear)\n",
      "        (proj_inter): Dense(None -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm_in): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "        (layer_norm_inter): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "      )\n",
      "      (3): TransformerDecoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell_in): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (attention_cell_inter): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (proj_in): Dense(None -> 512, linear)\n",
      "        (proj_inter): Dense(None -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm_in): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "        (layer_norm_inter): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "      )\n",
      "      (4): TransformerDecoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell_in): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (attention_cell_inter): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (proj_in): Dense(None -> 512, linear)\n",
      "        (proj_inter): Dense(None -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm_in): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "        (layer_norm_inter): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "      )\n",
      "      (5): TransformerDecoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell_in): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (attention_cell_inter): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(None -> 512, linear)\n",
      "          (proj_key): Dense(None -> 512, linear)\n",
      "          (proj_value): Dense(None -> 512, linear)\n",
      "        )\n",
      "        (proj_in): Dense(None -> 512, linear)\n",
      "        (proj_inter): Dense(None -> 512, linear)\n",
      "        (ffn): PositionwiseFFN(\n",
      "          (ffn_1): Dense(None -> 2048, linear)\n",
      "          (activation): Activation(relu)\n",
      "          (ffn_2): Dense(None -> 512, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=512)\n",
      "        )\n",
      "        (layer_norm_in): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "        (layer_norm_inter): LayerNorm(eps=1e-05, axis=-1, center=True, scale=True, in_channels=0)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (src_embed): HybridSequential(\n",
      "    (0): Embedding(358 -> 512, float32)\n",
      "    (1): Dropout(p = 0.0, axes=())\n",
      "  )\n",
      "  (tgt_embed): HybridSequential(\n",
      "    (0): Embedding(358 -> 512, float32)\n",
      "    (1): Dropout(p = 0.0, axes=())\n",
      "  )\n",
      "  (tgt_proj): Dense(None -> 381, linear)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use beam_size=4, alpha=0.60, K=5\n",
      "Use learning_rate=2.00\n",
      "Epoch 0 - train_one_epoch started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[Epoch 0 Batch 10/14] loss=24.8796, ppl=63836232372.9400, throughput=0.10K wps, wc=1.05K\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - train_one_epoch finished\n",
      "Epoch 0 - after waitall\n",
      "Epoch 0, valid Loss=16.8254, valid ppl=20286128.8392\n",
      "Epoch 0, test Loss=16.8254, test ppl=20286128.8392\n",
      "Epoch 1 - train_one_epoch started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[Epoch 1 Batch 10/14] loss=24.9889, ppl=71212615609.1284, throughput=0.00K wps, wc=1.10K\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - train_one_epoch finished\n",
      "Epoch 1 - after waitall\n",
      "Epoch 1, valid Loss=9.9820, valid ppl=21633.8037\n",
      "Epoch 1, test Loss=9.9820, test ppl=21633.8037\n",
      "Epoch 2 - train_one_epoch started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[Epoch 2 Batch 10/14] loss=10.8229, ppl=50156.4460, throughput=0.03K wps, wc=1.28K\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - train_one_epoch finished\n",
      "Epoch 2 - after waitall\n",
      "Epoch 2, valid Loss=6.0580, valid ppl=427.5138\n",
      "Epoch 2, test Loss=6.0580, test ppl=427.5138\n",
      "Best model valid Loss=7.3552, valid ppl=1564.3340\n",
      "Best model test Loss=7.3552, test ppl=1564.3340\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import horovod.mxnet as hvd\n",
    "import gluonnlp as nlp\n",
    "import nmt\n",
    "import dataprocessor\n",
    "import logging\n",
    "import argparse\n",
    "import time\n",
    "import utils\n",
    "import random\n",
    "import hyperparameters as hparams\n",
    "\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "ctx = mx.cpu(0)\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='Tranformer')\n",
    "\n",
    "parser.add_argument('--batch-size', type=int, default=64,\n",
    "                    help='training batch size (default: 64)')\n",
    "parser.add_argument('--dtype', type=str, default='float32',\n",
    "                    help='training data type (default: float32)')\n",
    "parser.add_argument('--epochs', type=int, default=5,\n",
    "                    help='number of training epochs (default: 5)')\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                    help='SGD momentum (default: 0.9)')\n",
    "parser.add_argument('--no-cuda', type=bool, default=False,\n",
    "                    help='disable training on GPU (default: False)')\n",
    "#args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "#args, _  = parser.parse_known_args()\n",
    "#hparams={\n",
    "#        'src_lang' : 'en',\n",
    "#        'tgt_lang' : 'de'\n",
    "#        }\n",
    "\n",
    "#args.no_cuda = False\n",
    "\n",
    "if not args.no_cuda:\n",
    "    # Disable CUDA if there are no GPUs.\n",
    "    if mx.context.num_gpus() == 0:\n",
    "        args.no_cuda = True\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.info(args)\n",
    "\n",
    "# Initialize Horovod\n",
    "hvd.init()\n",
    "\n",
    "# Set context to current process \n",
    "#ctx = mx.cpu(hvd.local_rank()) if args.no_cuda else mx.gpu(hvd.local_rank())\n",
    "\n",
    "num_workers = hvd.size()\n",
    "\n",
    "'''wmt_model_name = 'transformer_en_de_512'\n",
    "wmt_transformer_model, wmt_src_vocab, wmt_tgt_vocab = \\\n",
    "    nmt.transformer.get_model(wmt_model_name,\n",
    "                              dataset_name='WMT2014',\n",
    "                              pretrained=True,\n",
    "                              ctx=ctx)\n",
    "print(wmt_src_vocab)\n",
    "print(wmt_tgt_vocab)'''\n",
    "\n",
    "# Build model\n",
    "'''encoder, decoder = nmt.transformer.get_transformer_encoder_decoder(units=hparams.num_units,\n",
    "                                                   hidden_size=hparams.hidden_size,\n",
    "                                                   dropout=hparams.dropout,\n",
    "                                                   num_layers=hparams.num_layers,\n",
    "                                                   num_heads=hparams.num_heads,\n",
    "                                                   max_src_length=530,\n",
    "                                                   max_tgt_length=549,\n",
    "                                                   scaled=hparams.scaled)\n",
    "model = nmt.translation.NMTModel(src_vocab=src_vocab, tgt_vocab=tgt_vocab, encoder=encoder, decoder=decoder,\n",
    "                 share_embed=True, embed_size=hparams.num_units, tie_weights=True,\n",
    "                 embed_initializer=None, prefix='transformer_')\n",
    "model.initialize(init=mx.init.Xavier(magnitude=3.0), ctx=ctx)\n",
    "model.hybridize()'''\n",
    "\n",
    "\n",
    "\n",
    "'''wmt_data_test = nlp.data.WMT2014BPE('newstest2014',\n",
    "                                    src_lang=hparams.src_lang,\n",
    "                                    tgt_lang=hparams.tgt_lang,\n",
    "                                    full=False)\n",
    "print('Source language %s, Target language %s' % (hparams.src_lang, hparams.tgt_lang))\n",
    "wmt_data_test[0]\n",
    "\n",
    "wmt_test_text = nlp.data.WMT2014('newstest2014',\n",
    "                                 src_lang=hparams.src_lang,\n",
    "                                 tgt_lang=hparams.tgt_lang,\n",
    "                                 full=False)\n",
    "wmt_test_text[0]'''\n",
    "\n",
    "demo = True\n",
    "if demo:\n",
    "    dataset = 'TOY'\n",
    "else:\n",
    "    dataset = 'WMT2014BPE'\n",
    "\n",
    "data_train, data_val, data_test, val_tgt_sentences, test_tgt_sentences, src_vocab, tgt_vocab = \\\n",
    "    dataprocessor.load_translation_data(\n",
    "        dataset=dataset,\n",
    "        src_lang=hparams.src_lang,\n",
    "        tgt_lang=hparams.tgt_lang)\n",
    "\n",
    "data_train_lengths = dataprocessor.get_data_lengths(data_train)\n",
    "data_val_lengths = dataprocessor.get_data_lengths(data_val)\n",
    "data_test_lengths = dataprocessor.get_data_lengths(data_test)\n",
    "\n",
    "data_train = data_train.transform(lambda src, tgt: (src, tgt, len(src), len(tgt)), lazy=False)\n",
    "data_val = gluon.data.SimpleDataset([(ele[0], ele[1], len(ele[0]), len(ele[1]), i)\n",
    "                           for i, ele in enumerate(data_val)])\n",
    "data_test = gluon.data.SimpleDataset([(ele[0], ele[1], len(ele[0]), len(ele[1]), i)\n",
    "                           for i, ele in enumerate(data_test)])\n",
    "\n",
    "# Create optimizer\n",
    "optimizer_params = {'momentum': args.momentum,\n",
    "                    'learning_rate': args.lr * hvd.size()}\n",
    "opt = mx.optimizer.create('sgd', **optimizer_params)\n",
    "\n",
    "train_batchify_fn = nlp.data.batchify.Tuple(\n",
    "    nlp.data.batchify.Pad(),\n",
    "    nlp.data.batchify.Pad(),\n",
    "    nlp.data.batchify.Stack(dtype='float32'),\n",
    "    nlp.data.batchify.Stack(dtype='float32'))\n",
    "test_batchify_fn = nlp.data.batchify.Tuple(\n",
    "    nlp.data.batchify.Pad(),\n",
    "    nlp.data.batchify.Pad(),\n",
    "    nlp.data.batchify.Stack(dtype='float32'),\n",
    "    nlp.data.batchify.Stack(dtype='float32'),\n",
    "    nlp.data.batchify.Stack())\n",
    "\n",
    "target_val_lengths = list(map(lambda x: x[-1], data_val_lengths))\n",
    "target_test_lengths = list(map(lambda x: x[-1], data_test_lengths))\n",
    "\n",
    "bucket_scheme = nlp.data.ExpWidthBucket(bucket_len_step=1.2)\n",
    "train_batch_sampler = nlp.data.FixedBucketSampler(lengths=data_train_lengths,\n",
    "                                             batch_size=hparams.batch_size,\n",
    "                                             num_buckets=hparams.num_buckets,\n",
    "                                             ratio=0.0,\n",
    "                                             shuffle=True,\n",
    "                                             use_average_length=True,\n",
    "                                             num_shards=1,\n",
    "                                             bucket_scheme=bucket_scheme)\n",
    "print('Train Batch Sampler:')\n",
    "print(train_batch_sampler.stats())\n",
    "\n",
    "val_batch_sampler = nlp.data.FixedBucketSampler(lengths=target_val_lengths,\n",
    "                                       batch_size=hparams.test_batch_size,\n",
    "                                       num_buckets=hparams.num_buckets,\n",
    "                                       ratio=0.0,\n",
    "                                       shuffle=False,\n",
    "                                       use_average_length=True,\n",
    "                                       bucket_scheme=bucket_scheme)\n",
    "print('Validation Batch Sampler:')\n",
    "print(val_batch_sampler.stats())\n",
    "\n",
    "test_batch_sampler = nlp.data.FixedBucketSampler(lengths=target_test_lengths,\n",
    "                                        batch_size=hparams.test_batch_size,\n",
    "                                        num_buckets=hparams.num_buckets,\n",
    "                                        ratio=0.0,\n",
    "                                        shuffle=False,\n",
    "                                        use_average_length=True,\n",
    "                                        bucket_scheme=bucket_scheme)\n",
    "print('Test Batch Sampler:')\n",
    "print(test_batch_sampler.stats())\n",
    "\n",
    "train_data_loader = nlp.data.ShardedDataLoader(data_train,\n",
    "                                      batch_sampler=train_batch_sampler,\n",
    "                                      batchify_fn=train_batchify_fn,\n",
    "                                      num_workers=8)\n",
    "print('Length of train_data_loader: %d' % len(train_data_loader))\n",
    "val_data_loader = gluon.data.DataLoader(data_val,\n",
    "                             batch_sampler=val_batch_sampler,\n",
    "                             batchify_fn=test_batchify_fn,\n",
    "                             num_workers=8)\n",
    "print('Length of val_data_loader: %d' % len(val_data_loader))\n",
    "test_data_loader = gluon.data.DataLoader(data_test,\n",
    "                              batch_sampler=test_batch_sampler,\n",
    "                              batchify_fn=test_batchify_fn,\n",
    "                              num_workers=8)\n",
    "print('Length of test_data_loader: %d' % len(test_data_loader))\n",
    "\n",
    "# Build model\n",
    "encoder, decoder, one_step_ahead_decoder = nlp.model.transformer.get_transformer_encoder_decoder(units=hparams.num_units,\n",
    "                                                   hidden_size=hparams.hidden_size,\n",
    "                                                   dropout=hparams.dropout,\n",
    "                                                   num_layers=hparams.num_layers,\n",
    "                                                   num_heads=hparams.num_heads,\n",
    "                                                   max_src_length=530,\n",
    "                                                   max_tgt_length=549,\n",
    "                                                   scaled=hparams.scaled)\n",
    "model = nlp.model.translation.NMTModel(src_vocab=src_vocab, tgt_vocab=tgt_vocab, encoder=encoder, decoder=decoder,\n",
    "                 one_step_ahead_decoder=one_step_ahead_decoder, share_embed=True, embed_size=hparams.num_units, tie_weights=False,\n",
    "                 embed_initializer=None, prefix='transformer_')\n",
    "model.initialize(init=mx.init.Xavier(magnitude=3.0), ctx=ctx)\n",
    "model.hybridize()\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Fetch and broadcast parameters\n",
    "params = model.collect_params()\n",
    "if params is not None:\n",
    "    hvd.broadcast_parameters(params, root_rank=0)\n",
    "\n",
    "# Create DistributedTrainer, a subclass of gluon.Trainer\n",
    "trainer = hvd.DistributedTrainer(params,opt)\n",
    "\n",
    "label_smoothing = nlp.loss.LabelSmoothing(epsilon=hparams.epsilon, units=len(tgt_vocab))\n",
    "label_smoothing.hybridize()\n",
    "\n",
    "loss_function = nlp.loss.MaskedSoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "loss_function.hybridize()\n",
    "\n",
    "test_loss_function = nlp.loss.MaskedSoftmaxCrossEntropyLoss()\n",
    "test_loss_function.hybridize()\n",
    "\n",
    "detokenizer = nlp.data.SacreMosesDetokenizer()\n",
    "\n",
    "translator = nmt.translation.BeamSearchTranslator(model=model,\n",
    "                                                  beam_size=hparams.beam_size,\n",
    "                                                  scorer=nlp.model.BeamSearchScorer(alpha=hparams.lp_alpha,\n",
    "                                                                                    K=hparams.lp_k),\n",
    "                                                  max_length=200)\n",
    "print('Use beam_size=%d, alpha=%.2f, K=%d' % (hparams.beam_size, hparams.lp_alpha, hparams.lp_k))\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), hparams.optimizer,\n",
    "                        {'learning_rate': hparams.lr, 'beta2': 0.98, 'epsilon': 1e-9})\n",
    "print('Use learning_rate=%.2f' % (trainer.learning_rate))\n",
    "\n",
    "# Train model\n",
    "best_valid_loss = float('Inf')\n",
    "step_num = 0\n",
    "#We use warmup steps as introduced in [1].\n",
    "warmup_steps = hparams.warmup_steps\n",
    "grad_interval = hparams.num_accumulated\n",
    "model.collect_params().setattr('grad_req', 'add')\n",
    "#We use Averaging SGD [2] to update the parameters.\n",
    "average_start = (len(train_data_loader) // grad_interval) * \\\n",
    "    (hparams.epochs - hparams.average_start)\n",
    "average_param_dict = {k: mx.nd.array([0]) for k, v in\n",
    "                                      model.collect_params().items()}\n",
    "update_average_param_dict = True\n",
    "model.collect_params().zero_grad()\n",
    "for epoch_id in range(hparams.epochs):\n",
    "    print('Epoch %d - train_one_epoch started' % (epoch_id))\n",
    "    utils.train_one_epoch(epoch_id, model, train_data_loader, trainer,\n",
    "                          label_smoothing, loss_function, grad_interval,\n",
    "                          average_param_dict, update_average_param_dict,\n",
    "                          step_num, ctx)\n",
    "    print('Epoch %d - train_one_epoch finished' % (epoch_id))\n",
    "    mx.nd.waitall()\n",
    "    print('Epoch %d - after waitall' % (epoch_id))\n",
    "    # We define evaluation function as follows. The `evaluate` function use beam search translator\n",
    "    # to generate outputs for the validation and testing datasets.\n",
    "    valid_loss, _ = utils.evaluate(model, val_data_loader,\n",
    "                                   test_loss_function, translator,\n",
    "                                   tgt_vocab, detokenizer, ctx)\n",
    "    print('Epoch %d, valid Loss=%.4f, valid ppl=%.4f'\n",
    "          % (epoch_id, valid_loss, np.exp(valid_loss)))\n",
    "    test_loss, _ = utils.evaluate(model, test_data_loader,\n",
    "                                  test_loss_function, translator,\n",
    "                                  tgt_vocab, detokenizer, ctx)\n",
    "    print('Epoch %d, test Loss=%.4f, test ppl=%.4f'\n",
    "          % (epoch_id, test_loss, np.exp(test_loss)))\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        model.save_parameters('{}.{}'.format(hparams.save_dir, 'valid_best.params'))\n",
    "    model.save_parameters('{}.epoch{:d}.params'.format(hparams.save_dir, epoch_id))\n",
    "mx.nd.save('{}.{}'.format(hparams.save_dir, 'average.params'), average_param_dict)\n",
    "\n",
    "if hparams.average_start > 0:\n",
    "    for k, v in model.collect_params().items():\n",
    "        v.set_data(average_param_dict[k])\n",
    "else:\n",
    "    model.load_parameters('{}.{}'.format(hparams.save_dir, 'valid_best.params'), ctx)\n",
    "valid_loss, _ = utils.evaluate(model, val_data_loader,\n",
    "                               test_loss_function, translator,\n",
    "                               tgt_vocab, detokenizer, ctx)\n",
    "print('Best model valid Loss=%.4f, valid ppl=%.4f'\n",
    "      % (valid_loss, np.exp(valid_loss)))\n",
    "test_loss, _ = utils.evaluate(model, test_data_loader,\n",
    "                              test_loss_function, translator,\n",
    "                              tgt_vocab, detokenizer, ctx)\n",
    "print('Best model test Loss=%.4f, test ppl=%.4f'\n",
    "      % (test_loss, np.exp(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
